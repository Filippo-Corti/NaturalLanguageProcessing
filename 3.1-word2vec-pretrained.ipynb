{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Word2Vec using GenSim",
   "id": "6b09d3fb083344cf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-17T08:46:13.237514Z",
     "start_time": "2025-10-17T08:46:11.754847Z"
    }
   },
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pymongo\n",
    "import nltk\n",
    "import copy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial import distance\n",
    "\n",
    "nltk.download('punkt_tab')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/filippo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T08:46:31.450297Z",
     "start_time": "2025-10-17T08:46:13.297961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = pymongo.MongoClient()['cousine']\n",
    "recipes = db['foodcom']\n",
    "\n",
    "q = {}\n",
    "recipe_corpus = []\n",
    "size = recipes.count_documents(q)\n",
    "limit = 50_000\n",
    "\n",
    "for recipe in recipes.find(q).limit(limit):\n",
    "    try:\n",
    "        recipe_corpus.append(word_tokenize(recipe['description'].lower()))\n",
    "    except TypeError:\n",
    "        pass\n",
    "    except AttributeError:\n",
    "        pass"
   ],
   "id": "e54c10707389cdc1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T08:46:34.967267Z",
     "start_time": "2025-10-17T08:46:34.958925Z"
    }
   },
   "cell_type": "code",
   "source": "print(recipe_corpus[0])",
   "id": "a1309c2ebd03e529",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'cooking', 'light', ',', 'with', 'slight', 'midifications', '.', 'vegetarian', 'meal', 'hubby', 'will', 'actually', 'eat', '!']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T08:47:27.069172Z",
     "start_time": "2025-10-17T08:46:35.033339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "recipe_model = Word2Vec(\n",
    "    sentences=recipe_corpus,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=8,\n",
    "    epochs=25\n",
    ")"
   ],
   "id": "617a42de42aa7674",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Similarity with GenSim",
   "id": "2a77b22527a02fa6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T08:47:27.236645Z",
     "start_time": "2025-10-17T08:47:27.187364Z"
    }
   },
   "cell_type": "code",
   "source": "recipe_model.wv.most_similar('dinner')",
   "id": "8d853e6045ff24d7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supper', 0.6975187063217163),\n",
       " ('meal', 0.6011375188827515),\n",
       " ('dinners', 0.5321587920188904),\n",
       " ('brunch', 0.48907995223999023),\n",
       " ('entree', 0.4524165987968445),\n",
       " ('entertaining', 0.4512987434864044),\n",
       " ('lunch', 0.44626790285110474),\n",
       " ('superbowl', 0.44285741448402405),\n",
       " ('snack', 0.4113234281539917),\n",
       " ('buffet', 0.4104422330856323)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Compositionality with GenSim",
   "id": "b6beff7372e45cdf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T08:47:27.366009Z",
     "start_time": "2025-10-17T08:47:27.334768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dm = recipe_model.wv.doesnt_match(['pasta', 'spaghetti', 'noodles', 'apple'])\n",
    "common = recipe_model.wv.get_mean_vector(['pasta', 'spaghetti', 'noodles', 'risotto'])\n",
    "common_word = recipe_model.wv.similar_by_vector(common)\n",
    "analogy = recipe_model.wv.most_similar(positive=['pizza', 'steak'], negative=['tomato'])\n",
    "\n",
    "print(f\"Doesn't match: {dm}\")\n",
    "print(f\"Common terms: {common_word}\")\n",
    "print(f\"Analogy: {analogy}\")"
   ],
   "id": "39a547f5ccfa1f5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doesn't match: apple\n",
      "Common terms: [('pasta', 0.8309046030044556), ('spaghetti', 0.8067542314529419), ('noodles', 0.6823669075965881), ('risotto', 0.6756052374839783), ('linguine', 0.6583754420280457), ('penne', 0.5989437699317932), ('fettuccine', 0.5860188007354736), ('lasagna', 0.5693743228912354), ('ravioli', 0.5428127646446228), ('lasagne', 0.5396863222122192)]\n",
      "Analogy: [('flank', 0.44398728013038635), ('skirt', 0.43080782890319824), ('sirloin', 0.40152326226234436), ('steaks', 0.3841758370399475), ('grill', 0.37974080443382263), ('souvlaki', 0.3479692041873932), ('hamburgers', 0.3448116183280945), ('crust', 0.3362443149089813), ('pie', 0.3314985930919647), ('veal', 0.3248113691806793)]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Semantic Shift\n",
    "\n",
    "Moving from one corpus to another, how does the meaning of the words change?\n",
    "\n",
    "To do so, we can't just train two separate models and compare the embeddings. This is simply because dimensions do not necessarily match across models, thus the same word is like orthogonal across the two models.\n",
    "\n",
    "Instead, we have 3 options:\n",
    "1. Use math to transpose one space on to the other, to avoid the above explained issue.\n",
    "2. Compute relative similarity within each model and compare similarities. In this case, similarities work as a sort of normalization to avoid orthogonality issues.\n",
    "3. Tune a model on both corpuses, then fine-tune two versions of the model, one for each corpus (this is what is implemented below).\n"
   ],
   "id": "90e4f6f9429c2e5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:44:58.295272Z",
     "start_time": "2025-10-16T09:44:55.348558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "italian_q = {'search_terms': 'italian'}\n",
    "chinese_q = {'search_terms': 'chinese'}\n",
    "limit = 5_000\n",
    "italian_corpus = []\n",
    "chinese_corpus = []\n",
    "\n",
    "for q, c in [(italian_q, italian_corpus), (chinese_q, chinese_corpus)]:\n",
    "    for doc in recipes.find(q).limit(limit):\n",
    "        try:\n",
    "            tokens = word_tokenize(doc['description'].lower())\n",
    "            c.append(tokens)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "print(f\"Italian corpus: {len(italian_corpus)}, Chinese corpus: {len(chinese_corpus)}\")"
   ],
   "id": "ee58fd4c1d7cdee8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian corpus: 4922, Chinese corpus: 4173\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:45:25.051087Z",
     "start_time": "2025-10-16T09:45:06.367611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "main_corpus = italian_corpus + chinese_corpus\n",
    "m0 = Word2Vec(\n",
    "    sentences=main_corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=8,\n",
    "    epochs=50\n",
    ")"
   ],
   "id": "5855db4d2af8367b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:45:26.735569Z",
     "start_time": "2025-10-16T09:45:26.686126Z"
    }
   },
   "cell_type": "code",
   "source": "m0.wv.most_similar('dinner')",
   "id": "6b4934871e89ae88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meal', 0.6197167038917542),\n",
       " ('supper', 0.6117156147956848),\n",
       " ('lunch', 0.6020562648773193),\n",
       " ('guests', 0.520651638507843),\n",
       " ('informal', 0.499549001455307),\n",
       " ('picnics', 0.484683096408844),\n",
       " ('crowd', 0.48251673579216003),\n",
       " ('brunch', 0.4791308343410492),\n",
       " ('occasion', 0.47841671109199524),\n",
       " ('snack', 0.47509321570396423)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:46:10.755984Z",
     "start_time": "2025-10-16T09:45:51.634403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m_it = copy.deepcopy(m0)\n",
    "m_ch = copy.deepcopy(m0)\n",
    "\n",
    "# Fine tuning\n",
    "m_it.train(italian_corpus, total_examples=m0.corpus_count, epochs=m0.epochs)\n",
    "m_ch.train(chinese_corpus, total_examples=m0.corpus_count, epochs=m0.epochs)"
   ],
   "id": "119b70f4451145e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6120375, 8695900)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:46:31.879343Z",
     "start_time": "2025-10-16T09:46:31.873510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word = 'spaghetti'\n",
    "v0, vit, vch = m0.wv.get_vector(word), m_it.wv.get_vector(word), m_ch.wv.get_vector(word)\n",
    "\n",
    "print(f\"Moving to IT: {distance.cosine(vit, v0)}\")\n",
    "print(f\"Moving to CH: {distance.cosine(vch, v0)}\")\n",
    "print(f\"Moving from IT to CH: {distance.cosine(vch, vit)}\")"
   ],
   "id": "a73b1e586de80a9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving to IT: 0.04938506047944213\n",
      "Moving to CH: 0.0430384014382722\n",
      "Moving from IT to CH: 0.09096499077947129\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ac5f45d6588e0de3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
