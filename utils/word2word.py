import pandas as pd
import torch
from sklearn.metrics.pairwise import cosine_similarity
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from tqdm import tqdm

from utils.bow import Bow


class Word2WordPrediction(nn.Module):
    """
    A simple Neural Network with one hidden layer.
    Layers are linear, output performs softmax.
    
    Used to predict words from skip-grams
    """

    def __init__(self, input_dim, hidden_dim):
        super(Word2WordPrediction, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=False)
        self.fc2 = nn.Linear(hidden_dim, input_dim, bias=False)
        self.vectors = np.zeros((input_dim, hidden_dim))

    def forward(self, x):
        # Forward means run the network on the given input x
        hidden = self.fc1(x)
        output = self.fc2(hidden)
        output = F.softmax(output, dim=-1)
        return output

    def train(
            self,
            data_loader,
            epochs: int,
            learning_rate: float
    ):
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.parameters(), lr=learning_rate)
        loss_history = []

        for _ in tqdm(range(epochs), total=epochs):
            running_loss = 0.0

            for inputs, labels in data_loader:
                optimizer.zero_grad()
                outputs = self(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                running_loss += loss.item()

            loss_history.append(running_loss / len(data_loader))
        self._save_embeddings()
        return loss_history

    def get_vector(self, word_idx: int):
        return self.vectors[word_idx]

    def _save_embeddings(self):
        weights_fc1 = self.fc1.weight.data.detach().numpy()
        self.vectors = weights_fc1.T


class WordEmbeddings:
    """
    Utility class to play with Word Embeddings generated by a Word2WordPredictor
    starting from a BoW, processed using Skip-Grams.
    """

    def __init__(self, words: Bow, model: Word2WordPrediction):
        self.bow = words
        self.w2w = model

        # Extract W2W Predictor Embeddings
        embeddings = self.w2w.vectors
        self.sigma = cosine_similarity(embeddings, embeddings)

        # Pre-compute a similarity Matrix for W2W Embeddings
        self.sim = pd.DataFrame(
            self.sigma,
            index=self.bow.vocabulary,
            columns=self.bow.vocabulary
        )

    def __getitem__(self, word: str):
        return self.w2w.get_vector(self.bow[word])

    def most_similar(self, word: str, topk: int = 10):
        """Returns the top-K most similar words to the given word, by cosine similarity of their embeddings"""
        return self.sim.loc[word].sort_values(ascending=False).head(topk)

    def predict(self, word: str, topk: int = 10):
        """Returns the probability distribution of the next work after word"""
        vector = np.zeros(self.bow.size)
        vector[self.bow.word2idx[word]] = 1
        y_pred = pd.Series(
            self.w2w(torch.Tensor(vector)).detach().numpy(),
            index=self.bow.vocabulary
        ).sort_values(ascending=False).head(topk)
        return y_pred

    def vectors(self, words: list[str]):
        """Returns the embeddings of the words in the list"""
        return self.w2w.vectors[[self.bow[w] for w in words]]

    def analogy(self, a: str, b: str, c: str):
        """
        Finds x such that a : b = c : x.
        For example: man : king = woman : x should return 'queen'
        """
        positive = self.vectors([a, c]).sum(axis=0)
        negative = self.vectors([b]).sum(axis=0)
        answer = positive - negative
        sigma = cosine_similarity(np.array([answer]), self.w2w.vectors)
        i = np.argmax(sigma[0])
        return self.bow.idx2word[i], answer

    def vector_similarity(self, query: np.ndarray, topk: int = 10):
        sigma = cosine_similarity(np.array([query]), self.w2w.vectors)
        output = pd.Series(sigma[0], index=self.bow.vocabulary)
        return output.sort_values(ascending=False).head(topk)

    def search(self, positive: list[str], negative: list[str] = None,
               topk: int = 10):
        positive_v = self.vectors(positive).sum(axis=0)
        if negative is not None:
            negative_v = self.vectors(negative).sum(axis=0)
            answer_v = positive_v - negative_v
        else:
            answer_v = positive_v
        sigma = cosine_similarity(np.array([answer_v]), self.w2w.vectors)
        output = pd.Series(sigma[0], index=self.bow.vocabulary)
        return output.sort_values(ascending=False).head(topk)

    def spot_odd_one(self, words: list[str]):
        word_v = self.vectors(words=words)
        center_v = word_v.mean(axis=0)
        sigma = cosine_similarity(np.array([center_v]), word_v)
        return pd.Series(sigma[0], index=words).sort_values(ascending=True)

    def common_meanings(self, words: list[str], topk: int = 10):
        word_v = self.vectors(words=words)
        center_v = word_v.mean(axis=0)
        return self.vector_similarity(center_v, topk=topk)
