{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GPT\n",
    "\n",
    "**GPT (Generative Pre-trained Transformer)** is a *decoder-only* Transformer model trained to generate text *autoregressively*, meaning it predicts the next token in a sequence given all previous tokens.\n",
    "\n",
    "### Differences from the Standard Transformer Decoder\n",
    "Architecturally, GPT is almost identical to the decoder block from the original Transformer, with one key simplification:\n",
    "- GPT **removes the encoder-decoder attention layer**, since there is no encoder providing context.\n",
    "- It retains **masked self-attention**, ensuring each token can only attend to past tokens (no future information).\n",
    "- The feed-forward network, residual connections, and layer normalization are unchanged.\n",
    "\n",
    "In summary:\n",
    "> GPT = Transformer Decoder – (Encoder-Decoder Attention)\n",
    "\n",
    "### Training Objective\n",
    "GPT is trained as a **language model** using the *autoregressive* objective:\n",
    "$$\n",
    "P(x_1, x_2, ..., x_T) = \\prod_{t=1}^{T} P(x_t | x_1, ..., x_{t-1})\n",
    "$$\n",
    "At each step, the model receives a sequence of tokens and learns to predict the next one.\n",
    "Because there is no encoder, all contextual understanding is built from the left-to-right accumulation of information within the masked self-attention layers.\n",
    "\n",
    "\n",
    "![GPT](img/gpt.png)\n"
   ],
   "id": "c17c4624a49bc054"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-08T09:42:19.117130Z",
     "start_time": "2025-11-08T09:42:19.113228Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from jaxtyping import Float, Int\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "import einops\n",
    "import numpy as np\n",
    "import circuitsvis as cv\n",
    "from IPython.display import display, HTML"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:18:46.658117Z",
     "start_time": "2025-11-08T09:17:56.419838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "hooked_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False\n",
    ")"
   ],
   "id": "f19b2cd25ec3d943",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Filippo Corti\\miniconda3\\envs\\SocialMediaMining\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Filippo Corti\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Filippo Corti\\miniconda3\\envs\\SocialMediaMining\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Filippo Corti\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:18:50.646795Z",
     "start_time": "2025-11-08T09:18:50.642129Z"
    }
   },
   "cell_type": "code",
   "source": "device",
   "id": "2e7fd2458d770858",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:18:54.989511Z",
     "start_time": "2025-11-08T09:18:54.981658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"The raccoon sat on the mat.\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"Token (ids): {token_ids}\")\n",
    "print(f\"Tokens (string): {tokenizer.tokenize(text)}\")\n",
    "print(f\"Text string: {tokenizer.decode(token_ids, skip_special_tokens=False)}\")\n",
    "print(f\"Stuff to input a model: {tokenizer(text, return_tensors='pt')}\")"
   ],
   "id": "8d5db27d080c2023",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token (ids): [464, 3444, 20912, 3332, 319, 262, 2603, 13]\n",
      "Tokens (string): ['The', 'Ġrac', 'coon', 'Ġsat', 'Ġon', 'Ġthe', 'Ġmat', '.']\n",
      "Text string: The raccoon sat on the mat.\n",
      "Stuff to input a model: {'input_ids': tensor([[  464,  3444, 20912,  3332,   319,   262,  2603,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:19:25.417556Z",
     "start_time": "2025-11-08T09:19:24.967461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Once upon a\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    output_logits = gpt2(**input_ids)[\"logits\"]\n",
    "print(f\"Logits: {output_logits}\")\n",
    "print(f\"Logits shape: {output_logits.shape}\")"
   ],
   "id": "3e0fe5b9d7e28d3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[[ -34.5645,  -34.4081,  -38.3079,  ...,  -41.6996,  -39.7801,\n",
      "           -35.0521],\n",
      "         [ -84.7256,  -82.9327,  -87.0166,  ...,  -91.6668,  -86.2355,\n",
      "           -84.7095],\n",
      "         [-109.0798, -105.7259, -109.9116,  ..., -114.2847, -107.6934,\n",
      "          -105.3613]]], device='cuda:0')\n",
      "Logits shape: torch.Size([1, 3, 50257])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:19:27.141535Z",
     "start_time": "2025-11-08T09:19:27.117860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_probas = output_logits.softmax(dim=-1)\n",
    "print(f\"Probabilites over vocabulary: {output_probas}\")"
   ],
   "id": "7d8ea84e9909ac86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites over vocabulary: tensor([[[8.9157e-04, 1.0424e-03, 2.1106e-05,  ..., 7.1022e-07,\n",
      "          4.8419e-06, 5.4752e-04],\n",
      "         [3.2892e-06, 1.9759e-05, 3.3276e-07,  ..., 3.1810e-09,\n",
      "          7.2668e-07, 3.3428e-06],\n",
      "         [4.0889e-07, 1.1700e-05, 1.7799e-07,  ..., 2.2447e-09,\n",
      "          1.6359e-06, 1.6848e-05]]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:20:16.104932Z",
     "start_time": "2025-11-08T09:20:16.086491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "most_likely_next_tokens = tokenizer.batch_decode(output_logits.argmax(dim=-1)[0])\n",
    "print(list(zip(tokenizer.tokenize(text), most_likely_next_tokens)))"
   ],
   "id": "95574ac7a3cf596f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Once', ' the'), ('Ġupon', ' a'), ('Ġa', ' time')]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:20:17.800946Z",
     "start_time": "2025-11-08T09:20:17.796556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "next_token = output_logits[0, -1].argmax(dim=-1)\n",
    "next_char = tokenizer.decode(next_token)\n",
    "print(\n",
    "    \"The next token is:\", repr(next_char)\n",
    ")  # repr is to show special tokens and spaces\n",
    "print(\"How the sentence becomes: \", text + next_char)"
   ],
   "id": "40d43b1bdc5a8d56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next token is: ' time'\n",
      "How the sentence becomes:  Once upon a time\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:20:19.807188Z",
     "start_time": "2025-11-08T09:20:19.701082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize text\n",
    "text = \"Once upon a\"\n",
    "# Convert text to tensor format\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "print(\"Generating text...\\n\")\n",
    "# Generate 10 characters iteratively\n",
    "for i in range(10):\n",
    "    with torch.inference_mode():\n",
    "        # Get model predictions\n",
    "        output_logits = gpt2(**tokens).logits\n",
    "        # Select the most likely next token\n",
    "        next_token = output_logits[0, -1].argmax(dim=-1)\n",
    "        # Decode the token to a character\n",
    "        next_char = tokenizer.decode(next_token)\n",
    "    # Display the sequence so far\n",
    "    current_text = tokenizer.decode(tokens[\"input_ids\"][0])  # Reconstruct the string\n",
    "    print(f\"Generation step {i + 1}:\")\n",
    "    print(f\"Sequence so far: {current_text!r}\")\n",
    "    print(f\"{tokens['input_ids'].shape[-1] + 1}th char = {next_char!r}\\n\")\n",
    "    # Append the new character and re-tokenize\n",
    "    text += next_char\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "print(\"Final text:\", text)"
   ],
   "id": "f4c0cfe7e24b97dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "\n",
      "Generation step 1:\n",
      "Sequence so far: 'Once upon a'\n",
      "4th char = ' time'\n",
      "\n",
      "Generation step 2:\n",
      "Sequence so far: 'Once upon a time'\n",
      "5th char = ','\n",
      "\n",
      "Generation step 3:\n",
      "Sequence so far: 'Once upon a time,'\n",
      "6th char = ' the'\n",
      "\n",
      "Generation step 4:\n",
      "Sequence so far: 'Once upon a time, the'\n",
      "7th char = ' world'\n",
      "\n",
      "Generation step 5:\n",
      "Sequence so far: 'Once upon a time, the world'\n",
      "8th char = ' was'\n",
      "\n",
      "Generation step 6:\n",
      "Sequence so far: 'Once upon a time, the world was'\n",
      "9th char = ' a'\n",
      "\n",
      "Generation step 7:\n",
      "Sequence so far: 'Once upon a time, the world was a'\n",
      "10th char = ' place'\n",
      "\n",
      "Generation step 8:\n",
      "Sequence so far: 'Once upon a time, the world was a place'\n",
      "11th char = ' of'\n",
      "\n",
      "Generation step 9:\n",
      "Sequence so far: 'Once upon a time, the world was a place of'\n",
      "12th char = ' great'\n",
      "\n",
      "Generation step 10:\n",
      "Sequence so far: 'Once upon a time, the world was a place of great'\n",
      "13th char = ' beauty'\n",
      "\n",
      "Final text: Once upon a time, the world was a place of great beauty\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:20:25.905669Z",
     "start_time": "2025-11-08T09:20:25.738811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reference_text = \"Once upon a time, there was a fox who lived in a forest.\"\n",
    "tokens = hooked_gpt2.to_tokens(reference_text).to(device)\n",
    "logits, cache = hooked_gpt2.run_with_cache(tokens)\n",
    "html = cv.attention.attention_pattern(\n",
    "    tokens=hooked_gpt2.to_str_tokens(reference_text),\n",
    "    attention=cache[\"pattern\", 3][0][7],\n",
    ")\n",
    "styled_html = f\"\"\"\n",
    "<div style=\"width:800px; font-size:16px;\">\n",
    "    {html}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(styled_html))"
   ],
   "id": "bfc3d3c13dc6f6bf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "<div style=\"width:800px; font-size:16px;\">\n",
       "    <div id=\"circuits-vis-cc57a2c4-7c4f\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPattern } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-cc57a2c4-7c4f\",\n",
       "      AttentionPattern,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \"Once\", \" upon\", \" a\", \" time\", \",\", \" there\", \" was\", \" a\", \" fox\", \" who\", \" lived\", \" in\", \" a\", \" forest\", \".\"], \"attention\": [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7876810431480408, 0.21231898665428162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.599220335483551, 0.24833936989307404, 0.15244022011756897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.296779066324234, 0.06938503682613373, 0.4622218906879425, 0.17161405086517334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33528852462768555, 0.042065951973199844, 0.293761670589447, 0.2577070891857147, 0.07117675244808197, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11801104247570038, 0.020908525213599205, 0.07792358845472336, 0.09048682451248169, 0.6226000189781189, 0.0700700581073761, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17296171188354492, 0.042718060314655304, 0.02889428660273552, 0.024140192195773125, 0.4151094853878021, 0.3006802499294281, 0.01549601275473833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5042981505393982, 0.014261402189731598, 0.01721852272748947, 0.005787990987300873, 0.053527846932411194, 0.17142276465892792, 0.08965723216533661, 0.14382608234882355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23179036378860474, 0.0020897299982607365, 0.005951429717242718, 0.0004584075359161943, 0.04805496707558632, 0.10114488005638123, 0.0942310094833374, 0.3203880190849304, 0.19589121639728546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32498985528945923, 0.0022049867548048496, 0.0035724854096770287, 0.0021009750198572874, 0.023844556882977486, 0.05382927507162094, 0.039358384907245636, 0.17231988906860352, 0.23610390722751617, 0.1416756510734558, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06337903439998627, 1.4286633813753724e-05, 2.25071344175376e-05, 6.312016921583563e-05, 0.0008094447548501194, 0.0007012552814558148, 0.0015573110431432724, 0.009965731762349606, 0.023253059014678, 0.8893530964851379, 0.010881083086133003, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5446348786354065, 5.808942296425812e-05, 0.00013011839473620057, 0.00023981525737326592, 0.0008613320533186197, 0.0016119000501930714, 0.010681667365133762, 0.0024784193374216557, 0.012999828904867172, 0.27606505155563354, 0.0835856944322586, 0.06665316224098206, 0.0, 0.0, 0.0, 0.0], [0.2986809313297272, 1.2696902558673173e-05, 2.567870069469791e-05, 4.6431519876932725e-05, 0.0008937870152294636, 0.0007691167993471026, 0.0016731470823287964, 0.002142511773854494, 0.0043853577226400375, 0.1357896327972412, 0.08339647948741913, 0.3345252573490143, 0.1376589983701706, 0.0, 0.0, 0.0], [0.23188422620296478, 8.756719580560457e-06, 3.1033541745273396e-05, 9.260959814128e-06, 0.0006851314101368189, 0.0005689599784091115, 0.0006093382835388184, 0.0025763644371181726, 0.0008215181296691298, 0.04504666104912758, 0.048639632761478424, 0.48121196031570435, 0.12078703194856644, 0.06712009012699127, 0.0, 0.0], [0.3626531660556793, 2.645838503667619e-05, 6.353916978696361e-05, 0.00012310242163948715, 0.0003816935350187123, 0.0014199594734236598, 0.0007294395472854376, 0.0034003339242190123, 0.0036752454470843077, 0.01933738775551319, 0.05790482461452484, 0.15986783802509308, 0.12455049902200699, 0.1671973615884781, 0.09866917133331299, 0.0], [0.16244804859161377, 5.767114998889156e-05, 3.708156873472035e-05, 5.697577944374643e-05, 0.00023818237241357565, 0.0001929444697452709, 0.0007152047473937273, 0.0018311543390154839, 0.00275806593708694, 0.02968256175518036, 0.015158646740019321, 0.09902853518724442, 0.03359818458557129, 0.04684647172689438, 0.46186143159866333, 0.14548873901367188]]}\n",
       "    )\n",
       "    </script>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:25:13.168020Z",
     "start_time": "2025-11-08T09:25:13.164573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(gpt2)\n",
    "print(gpt2.config)"
   ],
   "id": "a45bd938eb8cce75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:25:13.774540Z",
     "start_time": "2025-11-08T09:25:13.771719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = torch.randn(shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    print(\"Output:\", output)\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = torch.randint(100, 1000, shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    print(\"Output:\", output)\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")"
   ],
   "id": "33786d77f0c2533e",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:25:14.686231Z",
     "start_time": "2025-11-08T09:25:14.682055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence = \"Once upon a time, \"\n",
    "tokenized_sequence = tokenizer.tokenize(sequence)\n",
    "tokens = tokenizer(sequence, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "print(\"Tokenized sequence:\", tokenized_sequence)\n",
    "print(\"Token IDs:\", tokens)"
   ],
   "id": "6912bb5a8713bfc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sequence: ['Once', 'Ġupon', 'Ġa', 'Ġtime', ',', 'Ġ']\n",
      "Token IDs: tensor([[7454, 2402,  257,  640,   11,  220]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:25:15.680472Z",
     "start_time": "2025-11-08T09:25:15.676029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = 1  # starting with only one batch (thus 1 sentence)\n",
    "seq_len = len(tokenized_sequence)  # 6\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    n_ctx: int = gpt2.config.n_ctx  # 1024\n",
    "    d_model: int = gpt2.config.n_embd  # hidden size, or embedding dimension\n",
    "    n_heads: int = gpt2.config.n_head  # number of attention heads\n",
    "    n_layers: int = gpt2.config.n_layer  # number of transformer blocks\n",
    "    d_mlp: int = 4 * d_model  # MLP hidden size, 3072\n",
    "    d_head: int = d_model // n_heads  # dimension of each attention head, 64\n",
    "    layer_norm_eps: float = gpt2.config.layer_norm_epsilon  # layer norm epsilon\n",
    "    d_vocab: int = gpt2.config.vocab_size  # number of tokens in the vocabulary\n",
    "    init_range: float = (\n",
    "        gpt2.config.initializer_range\n",
    "    )  # initialization range for weights\n",
    "    debug: bool = True\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ],
   "id": "867e93629a7892d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(n_ctx=1024, d_model=768, n_heads=12, n_layers=12, d_mlp=3072, d_head=64, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, debug=True)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:25:16.676677Z",
     "start_time": "2025-11-08T09:25:16.468618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "            self, int_tokens: Int[Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # just a mapping from int tokens to float vectors\n",
    "        return self.W_E[int_tokens]\n",
    "\n",
    "rand_int_test(Embed, [batch, seq_len])"
   ],
   "id": "280f46d9e42696fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 6])\n",
      "Output: tensor([[[ 0.0038, -0.0055, -0.0157,  ...,  0.0031,  0.0222, -0.0032],\n",
      "         [ 0.0239,  0.0089,  0.0070,  ..., -0.0113, -0.0085, -0.0087],\n",
      "         [-0.0225, -0.0015, -0.0037,  ...,  0.0027, -0.0052,  0.0158],\n",
      "         [ 0.0408, -0.0259,  0.0038,  ..., -0.0022, -0.0025,  0.0281],\n",
      "         [-0.0519,  0.0172, -0.0119,  ..., -0.0072,  0.0173, -0.0104],\n",
      "         [ 0.0233, -0.0210,  0.0044,  ..., -0.0187,  0.0101,  0.0057]]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "Output shape: torch.Size([1, 6, 768]) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:25:17.403356Z",
     "start_time": "2025-11-08T09:25:17.387919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "            self, int_tokens: Int[Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # take first seq_len learnt positional embeddings\n",
    "        batch, seq_len = int_tokens.shape\n",
    "        return einops.repeat(\n",
    "            self.W_pos[:seq_len],\n",
    "            \"seq_len d_model -> batch seq_len d_model\",\n",
    "            batch=batch  # 1\n",
    "        )\n",
    "\n",
    "rand_int_test(PosEmbed, [batch, seq_len])"
   ],
   "id": "820f4d53898411f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 6])\n",
      "Output: tensor([[[-0.0191, -0.0203,  0.0417,  ...,  0.0282, -0.0032,  0.0003],\n",
      "         [ 0.0268,  0.0016, -0.0034,  ..., -0.0117, -0.0140,  0.0083],\n",
      "         [-0.0132, -0.0190, -0.0066,  ...,  0.0175,  0.0058,  0.0253],\n",
      "         [-0.0289,  0.0164, -0.0315,  ..., -0.0021,  0.0537,  0.0199],\n",
      "         [-0.0102, -0.0250,  0.0203,  ...,  0.0277,  0.0105, -0.0184],\n",
      "         [ 0.0227, -0.0239,  0.0294,  ..., -0.0652,  0.0156, -0.0050]]],\n",
      "       device='cuda:0', grad_fn=<ExpandBackward0>)\n",
      "Output shape: torch.Size([1, 6, 768]) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:25:18.105108Z",
     "start_time": "2025-11-08T09:25:18.098154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.empty(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))  # Bias!\n",
    "        nn.init.normal_(self.w, std=cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "            self, embedding: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # compute mean\n",
    "        embedding_mean = embedding.mean(dim=-1, keepdim=True)\n",
    "        # compute standard deviation + eps\n",
    "        embedding_std = (embedding.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps).sqrt()\n",
    "        # compute normalized embedding\n",
    "        embedding = (embedding - embedding_mean) / embedding_std\n",
    "        return embedding * self.w + self.b\n",
    "\n",
    "\n",
    "rand_float_test(LayerNorm, [batch, seq_len, cfg.d_model])"
   ],
   "id": "d7a385d29db2c4f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 6, 768])\n",
      "Output: tensor([[[ 1.3676e-02,  1.9080e-02, -2.3117e-03,  ..., -5.4136e-03,\n",
      "          -2.7389e-03,  3.5097e-02],\n",
      "         [ 1.3717e-02, -3.8329e-05,  1.7807e-03,  ...,  1.5169e-03,\n",
      "           1.9697e-02,  2.9513e-02],\n",
      "         [ 1.8159e-02, -1.1157e-03, -1.0391e-02,  ...,  7.9021e-03,\n",
      "           2.1161e-03,  1.8344e-02],\n",
      "         [ 4.7796e-02, -6.9015e-03,  2.4161e-03,  ...,  2.5075e-02,\n",
      "           8.2112e-03,  1.5835e-02],\n",
      "         [ 1.4386e-02, -1.1134e-02,  5.8101e-03,  ...,  1.9905e-02,\n",
      "           1.6334e-03, -2.5775e-02],\n",
      "         [ 4.9066e-02,  3.0447e-02,  6.4585e-03,  ...,  6.0914e-04,\n",
      "           7.8220e-03, -1.8753e-02]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Output shape: torch.Size([1, 6, 768]) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:25:18.969304Z",
     "start_time": "2025-11-08T09:25:18.939926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_K = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.b_K = nn.Parameter(torch.zeros(cfg.n_heads, cfg.d_head))\n",
    "        self.W_Q = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.b_Q = nn.Parameter(torch.zeros(cfg.n_heads, cfg.d_head))\n",
    "        self.W_V = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_model, cfg.d_head))\n",
    "        self.b_V = nn.Parameter(torch.zeros(cfg.n_heads, cfg.d_head))\n",
    "        self.W_O = nn.Parameter(torch.empty(cfg.n_heads, cfg.d_head, cfg.d_model))\n",
    "        self.b_O = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "\n",
    "        nn.init.normal_(self.W_K, std=cfg.init_range)\n",
    "        nn.init.normal_(self.W_Q, std=cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=cfg.init_range)\n",
    "        self.mask = -1e4\n",
    "\n",
    "    def forward(\n",
    "            self, embedding: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        batch, seq_len, _ = embedding.shape\n",
    "        device = embedding.device\n",
    "        # compute K, Q, V projections\n",
    "        K = einops.einsum(\n",
    "            embedding,\n",
    "            self.W_K,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "        ) + self.b_K\n",
    "        Q = einops.einsum(\n",
    "            embedding,\n",
    "            self.W_Q,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "        ) + self.b_Q\n",
    "        V = einops.einsum(\n",
    "            embedding,\n",
    "            self.W_V,\n",
    "            \"batch seq_len d_model, n_heads d_model d_head -> batch seq_len n_heads d_head\"\n",
    "        ) + self.b_V\n",
    "        # compute attention scores\n",
    "        attention_scores = einops.einsum(\n",
    "            Q,\n",
    "            K,\n",
    "            \"batch dest_pos n_heads d_head, batch source_pos n_heads d_head -> batch n_heads dest_pos source_pos\"\n",
    "        )\n",
    "        # scale and mask attention scores (causal attention)\n",
    "        attention_scores = attention_scores / (self.cfg.d_head ** 0.5)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
    "        attention_scores = attention_scores.masked_fill(mask, self.mask) # Masked positions should be close to 0, not actually 0\n",
    "        # softmax attention scores\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        # compute weighted sum of values\n",
    "        z = einops.einsum(\n",
    "            V,\n",
    "            attention_scores,\n",
    "            \"batch seq_len n_heads d_head, batch n_heads dest_pos source_pos -> batch dest_pos n_heads d_head\"\n",
    "        )\n",
    "        # compute output projection\n",
    "        attn_output = einops.einsum(\n",
    "            z,\n",
    "            self.W_O,\n",
    "            \"batch seq_len n_heads d_head, n_heads d_head d_model -> batch seq_len d_model\"\n",
    "        ) + self.b_O\n",
    "        return attn_output\n",
    "\n",
    "rand_float_test(Attention, [batch, seq_len, cfg.d_model])"
   ],
   "id": "6af55e284945f7df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 6, 768])\n",
      "Output: tensor([[[ 0.7654,  0.3072, -0.0493,  ..., -0.1509, -1.6303,  0.0622],\n",
      "         [ 0.7654,  0.3072, -0.0493,  ..., -0.1509, -1.6303,  0.0622],\n",
      "         [ 0.7654,  0.3072, -0.0493,  ..., -0.1509, -1.6303,  0.0622],\n",
      "         [ 0.7654,  0.3072, -0.0493,  ..., -0.1509, -1.6303,  0.0622],\n",
      "         [ 0.7654,  0.3072, -0.0493,  ..., -0.1509, -1.6303,  0.0622],\n",
      "         [ 0.7654,  0.3072, -0.0493,  ..., -0.1509, -1.6303,  0.0622]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Output shape: torch.Size([1, 6, 768]) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:25:19.764960Z",
     "start_time": "2025-11-08T09:25:19.733283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gelu_new(\n",
    "    input: Float[torch.Tensor, \"batch pos d_mlp\"],\n",
    ") -> Float[torch.Tensor, \"batch pos d_mlp\"]:\n",
    "    # Implementation of GeLU used by GPT2 - subtly different from PyTorch's\n",
    "    return (\n",
    "        0.5\n",
    "        * input\n",
    "        * (\n",
    "            1.0\n",
    "            + torch.tanh(\n",
    "                np.sqrt(2.0 / np.pi) * (input + 0.044715 * torch.pow(input, 3.0))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty(cfg.d_model, cfg.d_mlp))\n",
    "        self.b_in = nn.Parameter(torch.zeros(cfg.d_mlp))\n",
    "        self.W_out = nn.Parameter(torch.empty(cfg.d_mlp, cfg.d_model))\n",
    "        self.b_out = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "        nn.init.normal_(self.W_in, std=cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=cfg.init_range)\n",
    "        self.activation = gelu_new\n",
    "\n",
    "    def forward(\n",
    "        self, embedding: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # compute in projection\n",
    "        pre = einops.einsum(\n",
    "            embedding,\n",
    "            self.W_in,\n",
    "            \"batch seq_len d_model, d_model d_mlp -> batch seq_len d_mlp\"\n",
    "        ) + self.b_in\n",
    "        # apply activation\n",
    "        post = self.activation(pre)\n",
    "        # compute out projection\n",
    "        mlp_out = einops.einsum(\n",
    "            post,\n",
    "            self.W_out,\n",
    "            \"batch seq_len d_mlp, d_mlp d_model -> batch seq_len d_model\"\n",
    "        ) + self.b_out\n",
    "        return mlp_out\n",
    "\n",
    "rand_float_test(MLP, [batch, seq_len, cfg.d_model])"
   ],
   "id": "5e33f75d146f786e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 6, 768])\n",
      "Output: tensor([[[-0.0502,  0.1272, -0.2881,  ...,  0.1292, -0.2617,  0.6294],\n",
      "         [-0.0928, -0.1346,  0.1020,  ..., -0.5522, -0.0493,  0.0119],\n",
      "         [ 0.6985,  0.4018, -0.5010,  ..., -0.0825, -0.3101,  0.0496],\n",
      "         [ 0.2202,  0.3617, -0.5777,  ...,  0.2528, -0.0769, -0.9744],\n",
      "         [-0.2822,  0.0543,  0.3357,  ...,  0.0627, -0.1648,  0.5498],\n",
      "         [ 0.4221,  0.4063, -0.0057,  ..., -0.1639, -0.3572,  0.6323]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Output shape: torch.Size([1, 6, 768]) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:25:20.622943Z",
     "start_time": "2025-11-08T09:25:20.573222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, input_embedding: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # normalize input\n",
    "        embedding = self.ln1(input_embedding)\n",
    "        # compute attention and add skip connection\n",
    "        mid_embedding = self.attn(embedding) + input_embedding\n",
    "        # normalize embedding\n",
    "        embedding = self.ln2(mid_embedding)\n",
    "        # compute MLP and add skip connection\n",
    "        output_embedding = self.mlp(embedding) + mid_embedding\n",
    "        # return output\n",
    "        return output_embedding\n",
    "\n",
    "rand_float_test(TransformerBlock, [batch, seq_len, cfg.d_model])"
   ],
   "id": "b816c2f7bc00073e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 6, 768])\n",
      "Output: tensor([[[ 1.9948, -1.0613, -0.2920,  ...,  0.1536,  0.4180,  0.0734],\n",
      "         [ 0.6076,  1.2586, -0.9310,  ..., -0.2288,  2.6938, -0.3181],\n",
      "         [ 0.2172, -0.1263, -1.1064,  ..., -0.4843,  0.1577,  0.5916],\n",
      "         [ 0.7809, -0.3711, -1.4346,  ...,  2.3915,  0.8297,  0.5332],\n",
      "         [-0.7466, -0.6414,  0.1979,  ...,  0.7472,  0.7787,  0.6182],\n",
      "         [-0.3397,  0.2590, -0.6169,  ..., -0.0495, -1.0076,  0.5021]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Output shape: torch.Size([1, 6, 768]) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:25:21.669004Z",
     "start_time": "2025-11-08T09:25:21.423108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(torch.empty(cfg.d_model, cfg.d_vocab))\n",
    "        self.b_U = nn.Parameter(torch.zeros(cfg.d_vocab))\n",
    "        nn.init.normal_(self.W_U, std=cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, embedding: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        # compute logits\n",
    "        logits = einops.einsum(\n",
    "            embedding,\n",
    "            self.W_U,\n",
    "            \"batch seq_len d_model, d_model d_vocab -> batch seq_len d_vocab\"\n",
    "        ) + self.b_U\n",
    "        return logits\n",
    "\n",
    "rand_float_test(Unembed, [batch, seq_len, cfg.d_model])"
   ],
   "id": "4a09221c83b4498c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 6, 768])\n",
      "Output: tensor([[[ 0.5838,  0.6147,  0.7070,  ..., -0.2241, -0.0592,  0.4205],\n",
      "         [-1.2668,  1.7735, -0.1435,  ...,  0.6636,  0.7671, -0.4350],\n",
      "         [ 0.2913, -1.0453, -0.6848,  ..., -0.1880,  0.4448, -0.1822],\n",
      "         [-0.1750, -0.2846, -0.5337,  ...,  0.1832,  0.0181, -0.2026],\n",
      "         [ 0.6756, -0.8161,  0.8145,  ...,  0.0566,  0.7791, -0.4561],\n",
      "         [ 0.2309,  0.6905,  0.0889,  ..., -0.4160, -0.3475,  0.3143]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Output shape: torch.Size([1, 6, 50257]) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:30:45.352152Z",
     "start_time": "2025-11-08T09:30:45.345807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, input_tokens: Int[Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        # compute embeddings + positional embeddings\n",
    "        embedding = self.embed(input_tokens) + self.pos_embed(input_tokens)\n",
    "        # compute transformer blocks outputs\n",
    "        for block in self.blocks:\n",
    "            embedding = block(embedding)\n",
    "        # normalize output\n",
    "        # compute logits\n",
    "        logits = self.unembed(self.ln_final(embedding))\n",
    "        return logits\n",
    "\n",
    "    def load_gpt2_weights(self, gpt2: GPT2LMHeadModel) -> None:\n",
    "        state_dict = {}\n",
    "\n",
    "        state_dict[\"embed.W_E\"] = gpt2.transformer.wte.weight\n",
    "        state_dict[\"pos_embed.W_pos\"] = gpt2.transformer.wpe.weight\n",
    "\n",
    "        for l in range(cfg.n_layers):\n",
    "            state_dict[f\"blocks.{l}.ln1.w\"] = gpt2.transformer.h[l].ln_1.weight\n",
    "            state_dict[f\"blocks.{l}.ln1.b\"] = gpt2.transformer.h[l].ln_1.bias\n",
    "\n",
    "            # In GPT-2, q,k,v are produced by one big linear map, whose output is\n",
    "            # concat([q, k, v])\n",
    "            W = gpt2.transformer.h[l].attn.c_attn.weight\n",
    "            W_Q, W_K, W_V = torch.tensor_split(W, 3, dim=1)\n",
    "            W_Q = einops.rearrange(W_Q, \"m (i h)->i m h\", i=cfg.n_heads)\n",
    "            W_K = einops.rearrange(W_K, \"m (i h)->i m h\", i=cfg.n_heads)\n",
    "            W_V = einops.rearrange(W_V, \"m (i h)->i m h\", i=cfg.n_heads)\n",
    "\n",
    "            state_dict[f\"blocks.{l}.attn.W_Q\"] = W_Q\n",
    "            state_dict[f\"blocks.{l}.attn.W_K\"] = W_K\n",
    "            state_dict[f\"blocks.{l}.attn.W_V\"] = W_V\n",
    "\n",
    "            qkv_bias = gpt2.transformer.h[l].attn.c_attn.bias\n",
    "            qkv_bias = einops.rearrange(\n",
    "                qkv_bias,\n",
    "                \"(qkv index head)->qkv index head\",\n",
    "                qkv=3,\n",
    "                index=cfg.n_heads,\n",
    "                head=cfg.d_head,\n",
    "            )\n",
    "            state_dict[f\"blocks.{l}.attn.b_Q\"] = qkv_bias[0]\n",
    "            state_dict[f\"blocks.{l}.attn.b_K\"] = qkv_bias[1]\n",
    "            state_dict[f\"blocks.{l}.attn.b_V\"] = qkv_bias[2]\n",
    "\n",
    "            W_O = gpt2.transformer.h[l].attn.c_proj.weight\n",
    "            W_O = einops.rearrange(W_O, \"(i h) m->i h m\", i=cfg.n_heads)\n",
    "            state_dict[f\"blocks.{l}.attn.W_O\"] = W_O\n",
    "            state_dict[f\"blocks.{l}.attn.b_O\"] = gpt2.transformer.h[l].attn.c_proj.bias\n",
    "\n",
    "            state_dict[f\"blocks.{l}.ln2.w\"] = gpt2.transformer.h[l].ln_2.weight\n",
    "            state_dict[f\"blocks.{l}.ln2.b\"] = gpt2.transformer.h[l].ln_2.bias\n",
    "\n",
    "            W_in = gpt2.transformer.h[l].mlp.c_fc.weight\n",
    "            state_dict[f\"blocks.{l}.mlp.W_in\"] = W_in\n",
    "            state_dict[f\"blocks.{l}.mlp.b_in\"] = gpt2.transformer.h[l].mlp.c_fc.bias\n",
    "\n",
    "            W_out = gpt2.transformer.h[l].mlp.c_proj.weight\n",
    "            state_dict[f\"blocks.{l}.mlp.W_out\"] = W_out\n",
    "            state_dict[f\"blocks.{l}.mlp.b_out\"] = gpt2.transformer.h[l].mlp.c_proj.bias\n",
    "        state_dict[\"unembed.W_U\"] = gpt2.lm_head.weight.T\n",
    "\n",
    "        # --- Fix: assicurarsi che il bias dell'unembed esista nello state_dict ---\n",
    "        if getattr(gpt2.lm_head, \"bias\", None) is not None:\n",
    "            state_dict[\"unembed.b_U\"] = gpt2.lm_head.bias.to(self.unembed.b_U.device)\n",
    "        else:\n",
    "            state_dict[\"unembed.b_U\"] = torch.zeros(\n",
    "                self.unembed.b_U.shape,\n",
    "                dtype=self.unembed.b_U.dtype,\n",
    "                device=self.unembed.b_U.device,\n",
    "            )\n",
    "\n",
    "        state_dict[\"ln_final.w\"] = gpt2.transformer.ln_f.weight\n",
    "        state_dict[\"ln_final.b\"] = gpt2.transformer.ln_f.bias\n",
    "        self.load_state_dict(state_dict)"
   ],
   "id": "26bae7a105c60a1a",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:30:47.025160Z",
     "start_time": "2025-11-08T09:30:45.754260Z"
    }
   },
   "cell_type": "code",
   "source": "rand_int_test(GPT, [batch, seq_len])",
   "id": "c2d980db34e41344",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 6])\n",
      "Output: tensor([[[-0.0068, -0.0099, -0.0056,  ..., -0.0149, -0.0012,  0.0143],\n",
      "         [-0.0130, -0.0090, -0.0041,  ..., -0.0160, -0.0105,  0.0134],\n",
      "         [-0.0067, -0.0079, -0.0072,  ..., -0.0215, -0.0041,  0.0113],\n",
      "         [-0.0089, -0.0034, -0.0052,  ..., -0.0209, -0.0040,  0.0203],\n",
      "         [-0.0058, -0.0114, -0.0069,  ..., -0.0173, -0.0086,  0.0145],\n",
      "         [-0.0156, -0.0124, -0.0033,  ..., -0.0148, -0.0093,  0.0163]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Output shape: torch.Size([1, 6, 50257]) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:30:47.659227Z",
     "start_time": "2025-11-08T09:30:47.090952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "demo_gpt2 = GPT(Config(debug=False)).to(device)\n",
    "demo_gpt2.load_gpt2_weights(gpt2)\n",
    "demo_gpt2.eval()\n",
    "#demo_gpt2.load_state_dict(hooked_gpt2.state_dict(), strict=False)"
   ],
   "id": "9860636a7505071a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embed): Embed()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm()\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:32:26.579514Z",
     "start_time": "2025-11-08T09:32:25.387464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize text\n",
    "text = \"Once upon a\"\n",
    "# Convert text to tensor format\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "# Use token ids directly to avoid re-tokenizing the decoded string each step\n",
    "input_ids = tokens[\"input_ids\"]  # shape (1, seq_len)\n",
    "print(\"Generating text...\\n\")\n",
    "# Generate 20 tokens iteratively\n",
    "for i in range(20):\n",
    "    with torch.inference_mode():\n",
    "        # Get model predictions for current sequence\n",
    "        output_logits = demo_gpt2(input_ids)\n",
    "        # Select the most likely next token id (scalar)\n",
    "        next_token_id = output_logits[0, -1].argmax(dim=-1)\n",
    "        # ensure shape (1,1) for concatenation\n",
    "        next_token_id = next_token_id.unsqueeze(0).unsqueeze(0).to(input_ids.device)\n",
    "    # Display the sequence so far (decode current token ids)\n",
    "    current_text = tokenizer.decode(input_ids[0].tolist())\n",
    "    print(f\"Generation step {i+1}:\")\n",
    "    print(f\"Sequence so far: {current_text!r}\")\n",
    "    print(f\"{input_ids.shape[-1]+1}th token id = {int(next_token_id.item())!r}\\n\")\n",
    "    # Append the new token id (autoregressive generation)\n",
    "    input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "# final decode\n",
    "final_text = tokenizer.decode(input_ids[0].tolist())\n",
    "print(\"Final text:\", final_text)"
   ],
   "id": "2a345991e0a6356e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "\n",
      "Generation step 1:\n",
      "Sequence so far: 'Once upon a'\n",
      "4th token id = 290\n",
      "\n",
      "Generation step 2:\n",
      "Sequence so far: 'Once upon a and'\n",
      "5th token id = 290\n",
      "\n",
      "Generation step 3:\n",
      "Sequence so far: 'Once upon a and and'\n",
      "6th token id = 290\n",
      "\n",
      "Generation step 4:\n",
      "Sequence so far: 'Once upon a and and and'\n",
      "7th token id = 290\n",
      "\n",
      "Generation step 5:\n",
      "Sequence so far: 'Once upon a and and and and'\n",
      "8th token id = 290\n",
      "\n",
      "Generation step 6:\n",
      "Sequence so far: 'Once upon a and and and and and'\n",
      "9th token id = 290\n",
      "\n",
      "Generation step 7:\n",
      "Sequence so far: 'Once upon a and and and and and and'\n",
      "10th token id = 290\n",
      "\n",
      "Generation step 8:\n",
      "Sequence so far: 'Once upon a and and and and and and and'\n",
      "11th token id = 3503\n",
      "\n",
      "Generation step 9:\n",
      "Sequence so far: 'Once upon a and and and and and and and etc'\n",
      "12th token id = 3503\n",
      "\n",
      "Generation step 10:\n",
      "Sequence so far: 'Once upon a and and and and and and and etc etc'\n",
      "13th token id = 3503\n",
      "\n",
      "Generation step 11:\n",
      "Sequence so far: 'Once upon a and and and and and and and etc etc etc'\n",
      "14th token id = 3503\n",
      "\n",
      "Generation step 12:\n",
      "Sequence so far: 'Once upon a and and and and and and and etc etc etc etc'\n",
      "15th token id = 3503\n",
      "\n",
      "Generation step 13:\n",
      "Sequence so far: 'Once upon a and and and and and and and etc etc etc etc etc'\n",
      "16th token id = 3503\n",
      "\n",
      "Generation step 14:\n",
      "Sequence so far: 'Once upon a and and and and and and and etc etc etc etc etc etc'\n",
      "17th token id = 3503\n",
      "\n",
      "Generation step 15:\n",
      "Sequence so far: 'Once upon a and and and and and and and etc etc etc etc etc etc etc'\n",
      "18th token id = 3503\n",
      "\n",
      "Generation step 16:\n",
      "Sequence so far: 'Once upon a and and and and and and and etc etc etc etc etc etc etc etc'\n",
      "19th token id = 3503\n",
      "\n",
      "Generation step 17:\n",
      "Sequence so far: 'Once upon a and and and and and and and etc etc etc etc etc etc etc etc etc'\n",
      "20th token id = 3503\n",
      "\n",
      "Generation step 18:\n",
      "Sequence so far: 'Once upon a and and and and and and and etc etc etc etc etc etc etc etc etc etc'\n",
      "21th token id = 3503\n",
      "\n",
      "Generation step 19:\n",
      "Sequence so far: 'Once upon a and and and and and and and etc etc etc etc etc etc etc etc etc etc etc'\n",
      "22th token id = 3503\n",
      "\n",
      "Generation step 20:\n",
      "Sequence so far: 'Once upon a and and and and and and and etc etc etc etc etc etc etc etc etc etc etc etc'\n",
      "23th token id = 3503\n",
      "\n",
      "Final text: Once upon a and and and and and and and etc etc etc etc etc etc etc etc etc etc etc etc etc\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:33:49.588451Z",
     "start_time": "2025-11-08T09:33:49.585524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assicurati che tutto sia su device e in modalità eval\n",
    "print(\"device models:\", device)\n",
    "print(\"demo_gpt2 device:\", next(demo_gpt2.parameters()).device)\n",
    "print(\"gpt2 device:\", next(gpt2.parameters()).device)\n",
    "print(\"demo_gpt2 eval?\", not demo_gpt2.training)\n",
    "print(\"gpt2 eval?\", not gpt2.training)\n",
    "# controlla shape di alcuni parametri chiave\n",
    "print(\"embed.W_E shape (demo):\", demo_gpt2.embed.W_E.shape)\n",
    "print(\"wte weight shape (hf):\", gpt2.transformer.wte.weight.shape)\n",
    "print(\"unembed.W_U shape (demo):\", demo_gpt2.unembed.W_U.shape)\n",
    "print(\"lm_head weight shape (hf):\", gpt2.lm_head.weight.shape)"
   ],
   "id": "d100357da680c9ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device models: cuda\n",
      "demo_gpt2 device: cuda:0\n",
      "gpt2 device: cuda:0\n",
      "demo_gpt2 eval? True\n",
      "gpt2 eval? True\n",
      "embed.W_E shape (demo): torch.Size([50257, 768])\n",
      "wte weight shape (hf): torch.Size([50257, 768])\n",
      "unembed.W_U shape (demo): torch.Size([768, 50257])\n",
      "lm_head weight shape (hf): torch.Size([50257, 768])\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:34:06.421417Z",
     "start_time": "2025-11-08T09:34:06.120906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Once upon a\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    hf_logits = gpt2(**inputs).logits  # shape (1, seq_len, vocab)\n",
    "    demo_logits = demo_gpt2(inputs[\"input_ids\"].long().to(next(demo_gpt2.parameters()).device))\n",
    "# confronta ultimi token (ultimo pos)\n",
    "hf_last = hf_logits[0, -1].float()\n",
    "demo_last = demo_logits[0, -1].float()\n",
    "print(\"hf_last mean/std:\", hf_last.mean().item(), hf_last.std().item())\n",
    "print(\"demo_last mean/std:\", demo_last.mean().item(), demo_last.std().item())\n",
    "diff = (hf_last - demo_last).abs()\n",
    "print(\"max abs diff:\", diff.max().item(), \"mean abs diff:\", diff.mean().item())\n",
    "\n",
    "# Top-5 tokens HF\n",
    "topk = 5\n",
    "hf_topk = torch.topk(hf_last, topk)\n",
    "demo_topk = torch.topk(demo_last, topk)\n",
    "print(\"\\nHF top-k tokens and scores:\")\n",
    "for i,t in enumerate(hf_topk.indices.tolist()):\n",
    "    print(i+1, tokenizer.decode([int(t)]), float(hf_topk.values[i]))\n",
    "print(\"\\nDEMO top-k tokens and scores:\")\n",
    "for i,t in enumerate(demo_topk.indices.tolist()):\n",
    "    print(i+1, tokenizer.decode([int(t)]), float(demo_topk.values[i]))"
   ],
   "id": "e50616bf470ff242",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_last mean/std: -109.96678924560547 3.1215107440948486\n",
      "demo_last mean/std: -97.36473846435547 3.5802812576293945\n",
      "max abs diff: 26.025436401367188 mean abs diff: 12.602171897888184\n",
      "\n",
      "HF top-k tokens and scores:\n",
      "1  time -96.10369110107422\n",
      "2  certain -96.84069061279297\n",
      "3  moment -97.83592224121094\n",
      "4  visit -97.91505432128906\n",
      "5  while -98.48494720458984\n",
      "\n",
      "DEMO top-k tokens and scores:\n",
      "1  and -81.44944763183594\n",
      "2 , -82.4196548461914\n",
      "3  or -82.89862823486328\n",
      "4  to -82.91533660888672\n",
      "5  all -83.01960754394531\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:39:11.620481Z",
     "start_time": "2025-11-08T09:39:11.615584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This will try to map keys and show shapes / mismatches between hf and demo params\n",
    "demo_state = demo_gpt2.state_dict()\n",
    "hf = gpt2.state_dict()\n",
    "\n",
    "mismatches = []\n",
    "missing_in_demo = []\n",
    "extra_in_demo = []\n",
    "\n",
    "# costruisco un mapping semplice per i nomi già usati nella funzione load_gpt2_weights\n",
    "# confronto shape se possibile\n",
    "for k_demo, v in demo_state.items():\n",
    "    if k_demo in hf:\n",
    "        if tuple(hf[k_demo].shape) != tuple(v.shape):\n",
    "            mismatches.append((k_demo, hf[k_demo].shape, v.shape))\n",
    "    else:\n",
    "        # prova a trovare alcune chiavi simili (utile per debugging)\n",
    "        pass\n",
    "\n",
    "# controlla per le key che ci aspettiamo nello state_dict che abbiamo costruito a mano\n",
    "expected_keys = list(demo_state.keys())\n",
    "print(\"Number of demo keys:\", len(demo_state))\n",
    "print(\"Number of hf keys:\", len(hf))\n",
    "print(\"Mismatches (shape differences):\", mismatches[:10])\n",
    "# Non andiamo a stampare tutto per non inquinare il notebook; se vuoi che controlli pattern di nomi,\n",
    "# stampa qui i primi 30 nomi delle state_dict per confronto:\n",
    "print(\"Demo keys sample:\", sorted(list(demo_state.keys())))\n",
    "print(\"HF keys sample:\", sorted(list(hf.keys())))"
   ],
   "id": "fd92aea8083eaaef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of demo keys: 198\n",
      "Number of hf keys: 149\n",
      "Mismatches (shape differences): []\n",
      "Demo keys sample: ['blocks.0.attn.W_K', 'blocks.0.attn.W_O', 'blocks.0.attn.W_Q', 'blocks.0.attn.W_V', 'blocks.0.attn.b_K', 'blocks.0.attn.b_O', 'blocks.0.attn.b_Q', 'blocks.0.attn.b_V', 'blocks.0.ln1.b', 'blocks.0.ln1.w', 'blocks.0.ln2.b', 'blocks.0.ln2.w', 'blocks.0.mlp.W_in', 'blocks.0.mlp.W_out', 'blocks.0.mlp.b_in', 'blocks.0.mlp.b_out', 'blocks.1.attn.W_K', 'blocks.1.attn.W_O', 'blocks.1.attn.W_Q', 'blocks.1.attn.W_V', 'blocks.1.attn.b_K', 'blocks.1.attn.b_O', 'blocks.1.attn.b_Q', 'blocks.1.attn.b_V', 'blocks.1.ln1.b', 'blocks.1.ln1.w', 'blocks.1.ln2.b', 'blocks.1.ln2.w', 'blocks.1.mlp.W_in', 'blocks.1.mlp.W_out', 'blocks.1.mlp.b_in', 'blocks.1.mlp.b_out', 'blocks.10.attn.W_K', 'blocks.10.attn.W_O', 'blocks.10.attn.W_Q', 'blocks.10.attn.W_V', 'blocks.10.attn.b_K', 'blocks.10.attn.b_O', 'blocks.10.attn.b_Q', 'blocks.10.attn.b_V', 'blocks.10.ln1.b', 'blocks.10.ln1.w', 'blocks.10.ln2.b', 'blocks.10.ln2.w', 'blocks.10.mlp.W_in', 'blocks.10.mlp.W_out', 'blocks.10.mlp.b_in', 'blocks.10.mlp.b_out', 'blocks.11.attn.W_K', 'blocks.11.attn.W_O', 'blocks.11.attn.W_Q', 'blocks.11.attn.W_V', 'blocks.11.attn.b_K', 'blocks.11.attn.b_O', 'blocks.11.attn.b_Q', 'blocks.11.attn.b_V', 'blocks.11.ln1.b', 'blocks.11.ln1.w', 'blocks.11.ln2.b', 'blocks.11.ln2.w', 'blocks.11.mlp.W_in', 'blocks.11.mlp.W_out', 'blocks.11.mlp.b_in', 'blocks.11.mlp.b_out', 'blocks.2.attn.W_K', 'blocks.2.attn.W_O', 'blocks.2.attn.W_Q', 'blocks.2.attn.W_V', 'blocks.2.attn.b_K', 'blocks.2.attn.b_O', 'blocks.2.attn.b_Q', 'blocks.2.attn.b_V', 'blocks.2.ln1.b', 'blocks.2.ln1.w', 'blocks.2.ln2.b', 'blocks.2.ln2.w', 'blocks.2.mlp.W_in', 'blocks.2.mlp.W_out', 'blocks.2.mlp.b_in', 'blocks.2.mlp.b_out', 'blocks.3.attn.W_K', 'blocks.3.attn.W_O', 'blocks.3.attn.W_Q', 'blocks.3.attn.W_V', 'blocks.3.attn.b_K', 'blocks.3.attn.b_O', 'blocks.3.attn.b_Q', 'blocks.3.attn.b_V', 'blocks.3.ln1.b', 'blocks.3.ln1.w', 'blocks.3.ln2.b', 'blocks.3.ln2.w', 'blocks.3.mlp.W_in', 'blocks.3.mlp.W_out', 'blocks.3.mlp.b_in', 'blocks.3.mlp.b_out', 'blocks.4.attn.W_K', 'blocks.4.attn.W_O', 'blocks.4.attn.W_Q', 'blocks.4.attn.W_V', 'blocks.4.attn.b_K', 'blocks.4.attn.b_O', 'blocks.4.attn.b_Q', 'blocks.4.attn.b_V', 'blocks.4.ln1.b', 'blocks.4.ln1.w', 'blocks.4.ln2.b', 'blocks.4.ln2.w', 'blocks.4.mlp.W_in', 'blocks.4.mlp.W_out', 'blocks.4.mlp.b_in', 'blocks.4.mlp.b_out', 'blocks.5.attn.W_K', 'blocks.5.attn.W_O', 'blocks.5.attn.W_Q', 'blocks.5.attn.W_V', 'blocks.5.attn.b_K', 'blocks.5.attn.b_O', 'blocks.5.attn.b_Q', 'blocks.5.attn.b_V', 'blocks.5.ln1.b', 'blocks.5.ln1.w', 'blocks.5.ln2.b', 'blocks.5.ln2.w', 'blocks.5.mlp.W_in', 'blocks.5.mlp.W_out', 'blocks.5.mlp.b_in', 'blocks.5.mlp.b_out', 'blocks.6.attn.W_K', 'blocks.6.attn.W_O', 'blocks.6.attn.W_Q', 'blocks.6.attn.W_V', 'blocks.6.attn.b_K', 'blocks.6.attn.b_O', 'blocks.6.attn.b_Q', 'blocks.6.attn.b_V', 'blocks.6.ln1.b', 'blocks.6.ln1.w', 'blocks.6.ln2.b', 'blocks.6.ln2.w', 'blocks.6.mlp.W_in', 'blocks.6.mlp.W_out', 'blocks.6.mlp.b_in', 'blocks.6.mlp.b_out', 'blocks.7.attn.W_K', 'blocks.7.attn.W_O', 'blocks.7.attn.W_Q', 'blocks.7.attn.W_V', 'blocks.7.attn.b_K', 'blocks.7.attn.b_O', 'blocks.7.attn.b_Q', 'blocks.7.attn.b_V', 'blocks.7.ln1.b', 'blocks.7.ln1.w', 'blocks.7.ln2.b', 'blocks.7.ln2.w', 'blocks.7.mlp.W_in', 'blocks.7.mlp.W_out', 'blocks.7.mlp.b_in', 'blocks.7.mlp.b_out', 'blocks.8.attn.W_K', 'blocks.8.attn.W_O', 'blocks.8.attn.W_Q', 'blocks.8.attn.W_V', 'blocks.8.attn.b_K', 'blocks.8.attn.b_O', 'blocks.8.attn.b_Q', 'blocks.8.attn.b_V', 'blocks.8.ln1.b', 'blocks.8.ln1.w', 'blocks.8.ln2.b', 'blocks.8.ln2.w', 'blocks.8.mlp.W_in', 'blocks.8.mlp.W_out', 'blocks.8.mlp.b_in', 'blocks.8.mlp.b_out', 'blocks.9.attn.W_K', 'blocks.9.attn.W_O', 'blocks.9.attn.W_Q', 'blocks.9.attn.W_V', 'blocks.9.attn.b_K', 'blocks.9.attn.b_O', 'blocks.9.attn.b_Q', 'blocks.9.attn.b_V', 'blocks.9.ln1.b', 'blocks.9.ln1.w', 'blocks.9.ln2.b', 'blocks.9.ln2.w', 'blocks.9.mlp.W_in', 'blocks.9.mlp.W_out', 'blocks.9.mlp.b_in', 'blocks.9.mlp.b_out', 'embed.W_E', 'ln_final.b', 'ln_final.w', 'pos_embed.W_pos', 'unembed.W_U', 'unembed.b_U']\n",
      "HF keys sample: ['lm_head.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.ln_f.bias', 'transformer.ln_f.weight', 'transformer.wpe.weight', 'transformer.wte.weight']\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:35:13.018268Z",
     "start_time": "2025-11-08T09:35:09.372686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Esempio semplice di greedy vs top-k sampling:\n",
    "def generate_demo(model, tokenizer, prompt, max_tokens=20, temperature=1.0, top_k=None):\n",
    "    model.eval()\n",
    "    ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(next(model.parameters()).device)\n",
    "    for _ in range(max_tokens):\n",
    "        with torch.inference_mode():\n",
    "            logits = model(ids)[0, -1]  # (vocab,)\n",
    "            logits = logits / max(temperature, 1e-8)\n",
    "            if top_k is not None:\n",
    "                values, indices = torch.topk(logits, top_k)\n",
    "                probs = torch.softmax(values, dim=-1)\n",
    "                next_id = indices[torch.multinomial(probs, num_samples=1)]\n",
    "            else:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                next_id = torch.multinomial(probs, num_samples=1)\n",
    "        ids = torch.cat([ids, next_id.unsqueeze(0)], dim=1)\n",
    "    return tokenizer.decode(ids[0].tolist())\n",
    "\n",
    "print(\"Greedy (temperature=1):\", generate_demo(demo_gpt2, tokenizer, \"Once upon a\", max_tokens=20, temperature=1.0, top_k=None))\n",
    "print(\"Top-k (k=40):\", generate_demo(demo_gpt2, tokenizer, \"Once upon a\", max_tokens=20, temperature=1.0, top_k=40))\n",
    "print(\"Sampling temp=0.7, k=40:\", generate_demo(demo_gpt2, tokenizer, \"Once upon a\", max_tokens=20, temperature=0.7, top_k=40))"
   ],
   "id": "31491e0f1651bb99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy (temperature=1): Once upon a oh or favorites under all RED forsoever light and plans light pending plans solicit plan pending plans plans plans\n",
      "Top-k (k=40): Once upon a & and and and under along and below and including links presently presently etc forthcomingnc etc avenue contact bid\n",
      "Sampling temp=0.7, k=40: Once upon a I, and and and and and forthcoming and forthcoming forthcoming forthcoming solicit solicit solicit solicit solicit solicit solicit solicit\n"
     ]
    }
   ],
   "execution_count": 69
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
