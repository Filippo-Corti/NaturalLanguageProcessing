{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Encoder-Decoder Architecture for seq2seq\n",
    "\n",
    "Task: translate Roman Numbers into Arabic Numbers\n",
    "\n",
    "\n"
   ],
   "id": "5d3af385a58e00d7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-27T14:53:17.921953Z",
     "start_time": "2025-10-27T14:53:17.914631Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import torch\n",
    "import utils.neural_nets as net\n",
    "from roman_arabic_numerals import conv"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build the Dataset and the DataLoader",
   "id": "11591d9cb6529745"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:25:36.070325Z",
     "start_time": "2025-10-27T14:25:36.020939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('./data/roman-numbers/classification.csv')\n",
    "data.head(2)"
   ],
   "id": "a5a24e09cb3a40ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     sequence target\n",
       "0  ___CXXXIII    odd\n",
       "1  ____CXCVII    odd"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>___CXXXIII</td>\n",
       "      <td>odd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>____CXCVII</td>\n",
       "      <td>odd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:25:44.531046Z",
     "start_time": "2025-10-27T14:25:44.511832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numbers = [x.replace('_', '') for x in data.sequence.values]\n",
    "arabic = [str(conv.rom_arab(n)) for n in numbers]\n",
    "numbers[:3], arabic[:3]"
   ],
   "id": "e8703a099cf96719",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['CXXXIII', 'CXCVII', 'XCIX'], ['133', '197', '99'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:29:23.274064Z",
     "start_time": "2025-10-27T14:29:23.257624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SOS_token = 0 # Start of String\n",
    "EOS_token = 1 # End of String\n",
    "\n",
    "class Lang:\n",
    "    \"\"\"Utility class that indexes tokens (for roman numbers, letters)\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.letter2index = {}\n",
    "        self.letter2count = {}\n",
    "        self.index2letter = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_letters = 2\n",
    "\n",
    "    def add_word(self, word):\n",
    "        for c in word:\n",
    "            self.add_letter(c)\n",
    "\n",
    "    def add_letter(self, letter):\n",
    "        if letter not in self.letter2index:\n",
    "            self.letter2index[letter] = self.n_letters\n",
    "            self.letter2count[letter] = 1\n",
    "            self.index2letter[self.n_letters] = letter\n",
    "            self.n_letters += 1\n",
    "        else:\n",
    "            self.letter2count[letter] += 1\n",
    "\n",
    "# Define two vocabularies\n",
    "roman_vocabulary, arabic_vocabulary = Lang(name='roman'), Lang('arabic')\n",
    "for i, roman in enumerate(numbers):\n",
    "    roman_vocabulary.add_word(roman)\n",
    "    arabic_vocabulary.add_word(arabic[i])"
   ],
   "id": "a46f8f738d0b9cc5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:40:33.347517Z",
     "start_time": "2025-10-27T14:40:33.321290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define into and target\n",
    "input_vocabulary = roman_vocabulary\n",
    "target_vocabulary = arabic_vocabulary\n",
    "\n",
    "def indexes_from_word(lang, word):\n",
    "    \"\"\"Translates a string into a sequence of integers, using lang\"\"\"\n",
    "    return [lang.letter2index[c] for c in word]\n",
    "\n",
    "def word_tensor(lang, word):\n",
    "    \"\"\"Produces the tensor of a word, using lang\"\"\"\n",
    "    indexes = indexes_from_word(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(1, -1)\n",
    "\n",
    "def pair_tensors(pair):\n",
    "    \"\"\"\"Produces tensors for a pair <input, target>\"\"\"\n",
    "    input_tensor = word_tensor(input_vocabulary, pair[0])\n",
    "    target_tensor = word_tensor(target_vocabulary, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    \"\"\"\n",
    "    Builds the Dataloader for the roman to arabic task.\n",
    "    The Dataloader returns pairs of tensors for <input, target> pairs.\n",
    "    \"\"\"\n",
    "    pairs = [(n, arabic[i]) for i, n in enumerate(numbers)]\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, net.MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, net.MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexes_from_word(input_lang, inp)\n",
    "        tgt_ids = indexes_from_word(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids), torch.LongTensor(target_ids))\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return roman_vocabulary, arabic_vocabulary, train_dataloader\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size=4)"
   ],
   "id": "5a33e3af93f166c8",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:40:32.235297Z",
     "start_time": "2025-10-27T14:40:32.224639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The mini batch is represented as 8 tensors:\n",
    "# - 4 tensors represent the inputs, with each input being a list of indexes\n",
    "# - 4 tensors represent the targets, with each input being a list of indexes\n",
    "for x in train_dataloader:\n",
    "    print(x)\n",
    "    break"
   ],
   "id": "c1ed0321a05c4ec0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[2, 6, 3, 3, 1, 0, 0, 0, 0, 0],\n",
      "        [2, 2, 3, 3, 5, 1, 0, 0, 0, 0],\n",
      "        [2, 3, 2, 5, 4, 4, 1, 0, 0, 0],\n",
      "        [2, 2, 3, 4, 3, 1, 0, 0, 0, 0]]), tensor([[ 2,  5, 10,  1,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  7,  6,  1,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2,  4,  5,  1,  0,  0,  0,  0,  0,  0],\n",
      "        [ 7,  2,  4,  1,  0,  0,  0,  0,  0,  0]])]\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train the Encoder-Decoder Network",
   "id": "f7518a2c23db3c0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:53:10.506135Z",
     "start_time": "2025-10-27T14:53:10.488827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(\n",
    "        dataloader,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        encoder_optimizer,\n",
    "        decoder_optimizer,\n",
    "        criterion\n",
    "):\n",
    "    \"\"\"Runs a training epoch over the entire dataset.\"\"\"\n",
    "    total_loss = 0\n",
    "    for input_tensor, target_tensor in dataloader:\n",
    "        # Make sure gradients have been reset\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        # Apply the encoder to embed the input sequence (the roman number)\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        # Apply the decoder to generate the corresponding arabic number (note that the decoder receives the hidden state of the encoder)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        # Gradient Descent and Back Propagation\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train(\n",
    "        train_dataloader,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        n_epochs,\n",
    "        learning_rate=0.001,\n",
    "        plot_every=100\n",
    "):\n",
    "    \"\"\"Trains the Encoder-Decoder Network on the Training DataLoader.\"\"\"\n",
    "    history = []\n",
    "    plot_loss_total = 0\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    run = list(range(n_epochs))\n",
    "    for epoch in tqdm(run):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            history.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    return history"
   ],
   "id": "c4fef2b736c553db",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run the Training\n",
    "\n",
    "hidden_size = 16\n",
    "batch_size = 4\n",
    "epochs = 50\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = net.EncoderRNN(input_lang.n_letters, hidden_size)\n",
    "decoder = net.DecoderRNN(hidden_size, output_lang.n_letters)\n",
    "\n",
    "history = train(train_dataloader, encoder, decoder, epochs, plot_every=5)"
   ],
   "id": "43bd6ce279643a8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "617700108657c65c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:11:00.689613Z",
     "start_time": "2025-10-27T15:11:00.680771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(encoder, decoder, word, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = word_tensor(input_lang, word=word)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_letters = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_letters.append('<EOS>')\n",
    "                break\n",
    "            decoded_letters.append(output_lang.index2letter[idx.item()])\n",
    "    return decoded_letters, decoder_attn"
   ],
   "id": "cabfbd91078c9616",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:11:35.084113Z",
     "start_time": "2025-10-27T15:11:35.065395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "test = 'XI'\n",
    "out, attn = evaluate(encoder, decoder, test, roman_vocabulary, arabic_vocabulary)\n",
    "print(out)"
   ],
   "id": "4338c883b7510ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<EOS>']\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:11:35.728642Z",
     "start_time": "2025-10-27T15:11:35.720057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attention = attn[0].detach().numpy()\n",
    "attention.shape, attention"
   ],
   "id": "5249835b6ebe03a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 3),\n",
       " array([[0.01353303, 0.16599102, 0.820476  ],\n",
       "        [0.09213254, 0.17835926, 0.72950816],\n",
       "        [0.01152237, 0.11995567, 0.8685219 ],\n",
       "        [0.0175138 , 0.13351867, 0.8489675 ],\n",
       "        [0.02106348, 0.13179219, 0.84714437],\n",
       "        [0.02053656, 0.12986624, 0.84959716],\n",
       "        [0.01949414, 0.12828569, 0.8522202 ],\n",
       "        [0.01855664, 0.12699355, 0.8544498 ],\n",
       "        [0.01780397, 0.12594481, 0.8562512 ],\n",
       "        [0.01721337, 0.12509772, 0.85768884]], dtype=float32))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2afbcb4dad98a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
