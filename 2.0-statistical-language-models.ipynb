{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Statistical Language Models\n",
    "\n"
   ],
   "id": "fffa7fe9721d4e29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T10:16:33.602995Z",
     "start_time": "2025-10-14T10:16:28.994550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pymongo\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "id": "10eff8685b7f909f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filippo/code/nlp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-14T10:16:33.641227Z",
     "start_time": "2025-10-14T10:16:33.615386Z"
    }
   },
   "source": [
    "class MarkovLM:\n",
    "    \"\"\"Implements a Markov LM\n",
    "    \"\"\"\n",
    "    def __init__(self, k: int = 2, tokenizer_model: str = \"dbmdz/bert-base-italian-uncased\"):\n",
    "        self.k = k\n",
    "        self.unigram = defaultdict(lambda: 1)\n",
    "        self.k_index = defaultdict(lambda: defaultdict(lambda: 1))\n",
    "        self.U = float('inf')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_model)\n",
    "        self.start_symbol = \"[#S]\"\n",
    "        self.end_symbol = \"[#E]\"\n",
    "\n",
    "    def train(self, corpus: List[str]):\n",
    "        \"\"\"fill if the indexes\n",
    "\n",
    "        Args:\n",
    "            corpus (List[str]): List of textual documents\n",
    "        \"\"\"\n",
    "        for document in tqdm(corpus):\n",
    "            try:\n",
    "                tokens = self.tokenizer.tokenize(document)\n",
    "                for keys in nltk.ngrams(tokens, n=self.k, pad_left=True,\n",
    "                                        pad_right=True,\n",
    "                                        left_pad_symbol=self.start_symbol,\n",
    "                                        right_pad_symbol=self.end_symbol):\n",
    "                    self.k_index[keys[:-1]][keys[-1]] += 1\n",
    "                    for k in keys:\n",
    "                        self.unigram[k] += 1\n",
    "            except TypeError:\n",
    "                pass\n",
    "\n",
    "    def pickup(self, prefix: tuple = None):\n",
    "        if prefix is None:\n",
    "            # unigram\n",
    "            s = pd.Series(self.unigram) / sum(self.unigram.values())\n",
    "            return np.random.choice(s.index.values, p=s.values)\n",
    "        else:\n",
    "            assert len(prefix) == self.k - 1\n",
    "            data = self.k_index[prefix]\n",
    "            s = pd.Series(data)\n",
    "            if s.empty:\n",
    "                token = self.pickup()\n",
    "            else:\n",
    "                s = s / s.sum()\n",
    "                token = np.random.choice(s.index.values, p=s.values)\n",
    "            return token\n",
    "\n",
    "    def generate(self, prefix: tuple = None, unigram: bool = False, max_len: int = 2000):\n",
    "        text = []\n",
    "        if prefix is None:\n",
    "            prefix = tuple([self.start_symbol] * (self.k - 1))\n",
    "        text.extend(prefix)\n",
    "        for i in range(max_len):\n",
    "            if unigram:\n",
    "                token = self.pickup()\n",
    "            else:\n",
    "                token = self.pickup(prefix=prefix)\n",
    "            text.append(token)\n",
    "            if token == self.end_symbol:\n",
    "                break\n",
    "            else:\n",
    "                prefix = tuple(text[-(self.k - 1):])\n",
    "        return text\n",
    "\n",
    "    def log_prob(self, text: str):\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        log_probs = []\n",
    "        for keys in nltk.ngrams(tokens, n=self.k, pad_left=True,\n",
    "                                pad_right=True,\n",
    "                                left_pad_symbol=self.start_symbol,\n",
    "                                right_pad_symbol=self.end_symbol):\n",
    "            prefix, next_word = keys[:-1], keys[-1]\n",
    "            try:\n",
    "                total = sum(self.k_index[prefix].values())\n",
    "                count = self.k_index[prefix][next_word]\n",
    "                log_p = np.log(count / total)\n",
    "                log_probs.append(log_p)\n",
    "            except KeyError:\n",
    "                log_probs.append(0)\n",
    "            except ZeroDivisionError:\n",
    "                log_probs.append(0)\n",
    "        return sum(log_probs)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_txt(file_path: str):\n",
    "        with open(file_path, 'r') as infile:\n",
    "            text = infile.read()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T10:16:33.700895Z",
     "start_time": "2025-10-14T10:16:33.680409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = pymongo.MongoClient()['cousine']\n",
    "recipes = db['foodcom']"
   ],
   "id": "c8269c533cccaf47",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T10:16:33.879199Z",
     "start_time": "2025-10-14T10:16:33.761674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_corpus(query:  dict = {}, numdocs: int = 3000):\n",
    "    corpus = []\n",
    "    for recipe in recipes.find(query).limit(numdocs):\n",
    "        for sentence in recipe['steps']:\n",
    "            corpus.append(sentence)\n",
    "    return corpus\n",
    "\n",
    "numdocs = 3000\n",
    "corpus = create_corpus(query={}, numdocs=numdocs)\n",
    "print(f\"Corpus size: {len(corpus)}\")\n",
    "for text in corpus[:4]:\n",
    "    print(text)"
   ],
   "id": "b98687b15e307311",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 20805\n",
      "To prepare ravioli, place mushrooms in food processor; pulse 10 times or until finely chopped.\n",
      "Heat oil and butter in a large nonstick skillet over medium-high heat. Add shallots and garlic, and sauté for 2 minutes.\n",
      "Add mushrooms and 1/8 teaspoon salt; cook 5 minutes or until moisture evaporates, stirring occasionally.\n",
      "Working with 1 wonton wrapper at a time (cover remaining wrappers with a damp towel to keep them from drying), spoon about 2 teaspoons mushroom mixture into center of each wrapper.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T10:16:35.081574Z",
     "start_time": "2025-10-14T10:16:35.076137Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = \"bert-base-uncased\"",
   "id": "2da1694f2d4e5c4c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T10:16:36.604972Z",
     "start_time": "2025-10-14T10:16:36.028192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "italian_q = {'search_terms': 'italian'}\n",
    "chinese_q = {'search_terms': 'chinese'}\n",
    "numdocs = 3000\n",
    "italian_corpus = create_corpus(query=italian_q, numdocs=numdocs)\n",
    "chinese_corpus = create_corpus(query=chinese_q, numdocs=numdocs)\n",
    "print(f\"Italian: {len(italian_corpus)}\")\n",
    "print(f\"Chinese: {len(chinese_corpus)}\")"
   ],
   "id": "7958035c24fe3349",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian: 31048\n",
      "Chinese: 24071\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T10:16:56.369489Z",
     "start_time": "2025-10-14T10:16:45.936868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "it = MarkovLM(k=4, tokenizer_model=tokenizer)\n",
    "ch = MarkovLM(k=4, tokenizer_model=tokenizer)"
   ],
   "id": "355132e1bcbbabf8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T10:17:22.713207Z",
     "start_time": "2025-10-14T10:16:56.381837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "it.train(corpus=italian_corpus)\n",
    "ch.train(corpus=chinese_corpus)"
   ],
   "id": "698c1d5805099e85",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31048/31048 [00:14<00:00, 2177.98it/s]\n",
      "100%|██████████| 24071/24071 [00:12<00:00, 1995.97it/s]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T10:18:04.341463Z",
     "start_time": "2025-10-14T10:18:04.328006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "italian_sentence = italian_corpus[6]\n",
    "chinese_sentence = chinese_corpus[6]\n",
    "\n",
    "print(f\"Italian sentence: {italian_sentence}\")\n",
    "print(f\"Italian: {it.log_prob(italian_sentence)}\")\n",
    "print(f\"Chinese: {ch.log_prob(italian_sentence)}\")\n",
    "print(\"========\")\n",
    "print(f\"Chinese sentence: {chinese_sentence}\")\n",
    "print(f\"Italian: {it.log_prob(chinese_sentence)}\")\n",
    "print(f\"Chinese: {ch.log_prob(chinese_sentence)}\")"
   ],
   "id": "8e73d2f7fb7f4ae6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian sentence: To prepare sauce, combine milk and flour in a small saucepan over medium-low heat; stir with a whisk.\n",
      "Italian: -35.87231088425833\n",
      "Chinese: -39.72849731483897\n",
      "========\n",
      "Chinese sentence: Cut bell pepper into thin strips and fry until the texture is soft.  Add scallions about 5 minutes after the bell pepper.\n",
      "Italian: -39.1592348306556\n",
      "Chinese: -39.93073368561632\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "752376afee14c29e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
