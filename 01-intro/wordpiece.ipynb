{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-02T14:09:51.048415Z",
     "start_time": "2025-10-02T14:09:51.043691Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T14:09:51.358287Z",
     "start_time": "2025-10-02T14:09:51.349474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_vocabulary(corpus, vocabulary, prefix):\n",
    "    for text in corpus:\n",
    "        for i, (a, b) in enumerate(nltk.ngrams(text, n=2)):\n",
    "            if i == 0:  # If it's the first char of the text\n",
    "                diad = (a, f\"{prefix}{b}\")\n",
    "            else:\n",
    "                diad = (f\"{prefix}{a}\", f\"{prefix}{b}\")\n",
    "            vocabulary[diad] += 1\n"
   ],
   "id": "9682365648ac9598",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T14:10:15.234265Z",
     "start_time": "2025-10-02T14:10:15.217817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = [\"play\", \"playing\", \"played\", \"plays\", \"playful\"]\n",
    "vocabulary = defaultdict(int)\n",
    "prefix = \"##\"\n",
    "\n",
    "update_vocabulary(\n",
    "    corpus,\n",
    "    vocabulary,\n",
    "    prefix\n",
    ")\n",
    "\n",
    "pd.Series(vocabulary).sort_values(ascending=False)"
   ],
   "id": "6fe2a3f3e94eebdf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p    ##l    5\n",
       "##l  ##a    5\n",
       "##a  ##y    5\n",
       "##y  ##i    1\n",
       "##i  ##n    1\n",
       "##n  ##g    1\n",
       "##y  ##e    1\n",
       "##e  ##d    1\n",
       "##y  ##s    1\n",
       "     ##f    1\n",
       "##f  ##u    1\n",
       "##u  ##l    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T14:14:44.180007Z",
     "start_time": "2025-10-02T14:14:44.165454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pair_to_merge = (pd.Series(vocabulary).sort_values(ascending=False).head(1).keys())[0]\n",
    "new_token = f\"{pair_to_merge[0].replace(prefix, '')}{pair_to_merge[1].replace(prefix, '')}\"\n",
    "\n",
    "vocabulary[new_token] = vocabulary[pair_to_merge]\n",
    "\n",
    "\n",
    "def wordpiece_split(word, vocabulary):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        matched = None\n",
    "        for j in range(len(word), i, -1):\n",
    "            subword = word[i:j]\n",
    "            if subword in vocabulary:\n",
    "                matched = subword\n",
    "                break\n",
    "        if matched is None:\n",
    "            matched = word[i]\n",
    "        tokens.append(matched)\n",
    "        i += len(matched)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "new_corpus = []\n",
    "for text in corpus:\n",
    "    tokens = wordpiece_split(text, vocabulary=vocabulary)\n",
    "    new_corpus.append(tokens)\n",
    "\n",
    "corpus = new_corpus"
   ],
   "id": "4da929c5b37a1a2c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T14:14:46.537244Z",
     "start_time": "2025-10-02T14:14:46.527931Z"
    }
   },
   "cell_type": "code",
   "source": "corpus",
   "id": "775d7374d7461bf5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pl', 'a', 'y'],\n",
       " ['pl', 'a', 'y', 'i', 'n', 'g'],\n",
       " ['pl', 'a', 'y', 'e', 'd'],\n",
       " ['pl', 'a', 'y', 's'],\n",
       " ['pl', 'a', 'y', 'f', 'u', 'l']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T14:14:48.722969Z",
     "start_time": "2025-10-02T14:14:48.711261Z"
    }
   },
   "cell_type": "code",
   "source": "vocabulary",
   "id": "bc3002db061bcb4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('p', '##l'): 5,\n",
       "             ('##l', '##a'): 5,\n",
       "             ('##a', '##y'): 5,\n",
       "             ('##y', '##i'): 1,\n",
       "             ('##i', '##n'): 1,\n",
       "             ('##n', '##g'): 1,\n",
       "             ('##y', '##e'): 1,\n",
       "             ('##e', '##d'): 1,\n",
       "             ('##y', '##s'): 1,\n",
       "             ('##y', '##f'): 1,\n",
       "             ('##f', '##u'): 1,\n",
       "             ('##u', '##l'): 1,\n",
       "             'pl': 5})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T14:16:11.680750Z",
     "start_time": "2025-10-02T14:16:11.669122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "update_vocabulary(corpus=corpus, vocabulary=vocabulary, prefix=prefix)\n",
    "pd.Series(vocabulary).sort_values(ascending=False)\n"
   ],
   "id": "735414dbb267c5b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(##a, ##y)    10\n",
       "(p, ##l)       5\n",
       "(##l, ##a)     5\n",
       "pl             5\n",
       "(pl, ##a)      5\n",
       "(##y, ##i)     2\n",
       "(##n, ##g)     2\n",
       "(##i, ##n)     2\n",
       "(##y, ##e)     2\n",
       "(##e, ##d)     2\n",
       "(##y, ##f)     2\n",
       "(##y, ##s)     2\n",
       "(##u, ##l)     2\n",
       "(##f, ##u)     2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T14:54:02.313536Z",
     "start_time": "2025-10-02T14:54:02.299860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WordPieceTokenizer:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_tokens: int\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        self.prefix = \"#\"\n",
    "        self.unknown = \"[UNK]\"\n",
    "        self.vocabulary = defaultdict(int)\n",
    "\n",
    "    def tokenize_text(\n",
    "            self,\n",
    "            text: str\n",
    "    ) -> list[list[str]]:\n",
    "\n",
    "        words = text.split()\n",
    "        vocab = self.init_vocab(text)\n",
    "\n",
    "        while len(vocab) < self.max_tokens:\n",
    "\n",
    "            # 1. Tokenize all words using current vocabulary\n",
    "            corpus = [self.tokenize_with_vocab(vocab, word) for word in words]\n",
    "\n",
    "            # 2. Get most frequent pair based on current corpus\n",
    "            mfp = self.get_most_frequent_pair(corpus)\n",
    "\n",
    "            # 3. Make the most frequent pair into a single token\n",
    "            first, second = mfp\n",
    "            new_token = first + second.replace(self.prefix, \"\")\n",
    "\n",
    "            # 4. Add the new token to the vocabulary\n",
    "            vocabulary.add(new_token)\n",
    "\n",
    "    def init_vocab(\n",
    "            self,\n",
    "            text: str\n",
    "    ) -> set[str]:\n",
    "        \"\"\" Initialize a vocabulary based on text \"\"\"\n",
    "        vocab = set()\n",
    "        for word in text.split():\n",
    "            for idx, char in enumerate(word):\n",
    "                token = char if idx == 0 else self.prefix + char\n",
    "                vocab.add(token)\n",
    "        return vocab\n",
    "\n",
    "    def get_most_frequent_pair(\n",
    "            self,\n",
    "            corpus: list[list[str]]\n",
    "    ) -> tuple[int, int]:\n",
    "        pairs_counter = defaultdict(int)\n",
    "        for word in corpus:\n",
    "            for a, b in nltk.ngrams(word, n=2):\n",
    "                pairs_counter[(a, b)] += 1\n",
    "        return max(pairs_counter, key=pairs_counter.get)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "b5d5f33443406679",
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'while' statement on line 21 (1602550312.py, line 24)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31mwhile len(vocabulary) < self.max_tokens:\u001B[39m\n    ^\n\u001B[31mIndentationError\u001B[39m\u001B[31m:\u001B[39m expected an indented block after 'while' statement on line 21\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
