{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-02T17:34:01.514806Z",
     "start_time": "2025-10-02T17:34:01.500299Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm # Progress bar"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T19:38:09.499467Z",
     "start_time": "2025-10-02T19:38:09.487812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WordPieceTokenizer:\n",
    "    def __init__(self, text: str, max_vocabulary_size: int):\n",
    "        self.text = text\n",
    "        self.max_vocabulary_size = max_vocabulary_size\n",
    "        self.prefix = \"#\"\n",
    "        self.unknown_token = \"[UNK]\"\n",
    "        self.corpus = list() # The text, tokenized using self.vocabulary\n",
    "        self.vocabulary = defaultdict(int) # The vocabulary of learned tokens and their frequency in self.text\n",
    "        self.init_corpus_and_vocabulary()\n",
    "\n",
    "    def init_corpus_and_vocabulary(self):\n",
    "        \"\"\"Initalizes corpus and vocabulary based on self.text\"\"\"\n",
    "        for word in self.text.split(): # For each word in the text\n",
    "            tokenized_word = list()\n",
    "            for idx, char in enumerate(word.strip()): # Split into default tokens (either x or #x)\n",
    "                token = char if idx == 0 else self.prefix + char\n",
    "                tokenized_word.append(token)\n",
    "                self.vocabulary[token] += 1\n",
    "            self.corpus.append(tokenized_word) # Then save the tokenized word into corpus\n",
    "\n",
    "    def train(self, normalize: bool = True):\n",
    "        for _ in tqdm(range(self.max_vocabulary_size)): # Equivalent to checking vocabulary size, as each cycle adds one vocabulary\n",
    "            check = self.update(normalize=normalize)\n",
    "            if not check: break\n",
    "\n",
    "    def get_token_pairs(self, normalize: bool = True) -> pd.Series:\n",
    "        scores = defaultdict(int) # The frequency (absolute or relative) of each pair\n",
    "        for tokenized_word in self.corpus:\n",
    "            for a, b in nltk.ngrams(tokenized_word, n=2): # Take 2-grams of the tokens in which the word was split\n",
    "                scores[(a, b)] += 1\n",
    "        if normalize:\n",
    "            scores = {\n",
    "                (a, b): score / (self.vocabulary[a] * self.vocabulary[b])\n",
    "                for (a, b), score in scores.items()\n",
    "            }\n",
    "        return pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    def update(self, normalize: bool = True) -> bool:\n",
    "        scores = self.get_token_pairs(normalize=normalize)\n",
    "        if scores.empty: return False\n",
    "\n",
    "        (a, b), score = list(scores.items())[0]\n",
    "        if len(self.vocabulary) >= self.max_vocabulary_size - 1:\n",
    "            return False\n",
    "\n",
    "        # Very lazy update (just re-tokenize every word after updating vocabulary)\n",
    "        self.vocabulary[a + b.replace(self.prefix, \"\")] += score\n",
    "        self.corpus = [self.tokenize_text(word) for word in self.text.split()]\n",
    "        return True\n",
    "\n",
    "    def tokenize_text(self, text: str) -> list[str]:\n",
    "        words = text.split()\n",
    "        return sum([self.tokenize_word(word) for word in words], [])\n",
    "\n",
    "    def tokenize_word(self, word: str) -> list[str]:\n",
    "        if word in self.vocabulary:\n",
    "            return [word]\n",
    "\n",
    "        tokens = list()\n",
    "        start = 0\n",
    "        while start < len(word):\n",
    "            end = len(word)\n",
    "            # Find the longest possible token\n",
    "            new_token = None\n",
    "            while start < end:\n",
    "                current_substring = word[start:end] if start == 0 else self.prefix + word[start:end]\n",
    "                if current_substring in self.vocabulary:\n",
    "                    new_token = current_substring\n",
    "                    break\n",
    "                end -= 1\n",
    "\n",
    "            if not new_token: # No token applicable (not even single chars)\n",
    "                return [self.unknown_token]\n",
    "\n",
    "            tokens.append(new_token)\n",
    "            start = end # Go to next char and repeat the search\n",
    "\n",
    "        return tokens\n"
   ],
   "id": "b5d5f33443406679",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T19:38:24.500002Z",
     "start_time": "2025-10-02T19:38:24.269917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "Years ago, the fearsome Pirate King, Gol D. Roger was executed leaving a huge pile of treasure and the famous \"One Piece\" behind.\n",
    "Whoever claims the \"One Piece\" will be named the new King of the Pirates.\n",
    "Monkey D. Luffy, a boy who consumed a \"Devil Fruit,\" decides to follow in the footsteps of his idol, the pirate Shanks, and find the One Piece.\n",
    "It helps, of course, that his body has the properties of rubber and that he's surrounded by a bevy of skilled fighters and thieves to help him along the way.\n",
    "Luffy will do anything to get the One Piece and become King of the Pirates!\n",
    "\"\"\"\n",
    "\n",
    "test = \"consumed a\"\n",
    "\n",
    "tokenizer = WordPieceTokenizer(text=text, max_vocabulary_size=180)\n",
    "tokenizer.train(normalize=False)\n",
    "\n",
    "tokenizer.tokenize_text(test)"
   ],
   "id": "edf215f539f9a9e7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 118/180 [00:00<00:00, 531.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['con', '#su', '#med', 'a']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bcaa158db574fc59"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
